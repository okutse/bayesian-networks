<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Amos Okutse, Naomi Lee" />


<title>Bayesian Learning Artificial Neural Networks for Modeling Survival Data</title>

<script src="index_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="index_files/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="index_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="index_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="index_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="index_files/navigation-1.1/tabsets.js"></script>
<script src="index_files/navigation-1.1/codefolding.js"></script>
<script src="index_files/kePrint-0.0.1/kePrint.js"></script>
<link href="index_files/lightable-0.0.1/lightable.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>



<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

.sourceCode .row {
  width: 100%;
}
.sourceCode {
  overflow-x: auto;
}
.code-folding-btn {
  margin-right: -30px;
}
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>




</head>

<body>


<div class="container-fluid main-container">




<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Bayesian Learning Artificial Neural Networks for Modeling Survival Data</h1>
<h4 class="author">Amos Okutse, Naomi Lee</h4>
<h4 class="date">May 14, 2022</h4>

</div>


<center>
<strong>Reading Time: 18 minute(s)</strong>
</center>
<p><br></p>
<figure>
<center>
<img src="intel.jpg" style="width:100%">
<figcaption style="font-style: italic; text-align: center;">
Image Source: <a href="https://iveybusinessjournal.com/the-reality-of-artificial-intelligence/">Eric Lee</a>
</figcaption>
</center>
</figure>
<div id="motivation" class="section level1" number="1">
<h1><span class="header-section-number" style="color:#4582ec">1</span> Motivation</h1>
<p>Accurate predictions of prognostic outcomes are of substantial and pivotal significance in the context of quality care delivery. However, the application of deep learning models to enhance caregiving in healthcare has been limited by concerns related to the reliability of such methods. In this way, models that are robust and which can result in a throughput prediction of such clinical outcomes as survival while at the same time exhibiting high reliability and potential to be generalized to larger populations remain in high demand. There has been an emerging persistent interest in modeling survival data to leverage the promise deep learning models offer in this regard. This is not surprising given the significance of the healthcare sector, where we are often interested in understanding, for instance, the role that a specific differentially expressed gene plays concerning prognosis or, more generally, understanding how a given treatment regimen is likely to impact patient outcomes and in turn make decisions accordingly to improve these patient outcomes related to care.</p>
<p>Analyzing time-to-event data involves is an inimitable problem given that the outcome of interest might comprise whether or not an event has occurred (binary outcome) but also the time when this event occurs (continuous outcome) <span class="citation">(<a href="#ref-feng2021bdnnsurv" role="doc-biblioref">Feng and Zhao 2021</a>)</span>. The problem is further complicated by missing data on the survival outcome of interest—censored data<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. The very nature of (censored) survival data makes it impossible to apply classical analysis methods such as logistic regression.</p>
<p>Additionally, models based on the Weibull model have restrictive assumptions, including a parametric form of the distribution of the time to event. Similarly, the semi-parametric Cox proportional hazards (PH) model <span class="citation">(<a href="#ref-burden2008bayesian" role="doc-biblioref">Burden and Winkler 2008</a>)</span> also has assumptions, a major one being the proportional hazards assumption: “the effect of a unit increase in a covariate is multiplicative with respect to the hazard rate.” Despite the outcome of interest not always being a hazard rate, it can be a probability; for instance, the PH assumption does not make much sense, especially when we have a substantial number of covariates (we would need each of these covariates to satisfy this assumption). The performance of these methods has also been shown to be poor, especially when the underlying model is incorrectly specified <span class="citation">(<a href="#ref-feng2021bdnnsurv" role="doc-biblioref">Feng and Zhao 2021</a>)</span>.</p>
<p>But how can we tackle the problem of modeling survival data amicably? This post reviews an extension of artificial neural networks (ANN) implemented based on the Cox PH model and trained to model survival data using Bayesian learning. In particular, we use a 2-layer feed-forward artificial neural network (ANN) trained using Bayesian inference to model survival outcomes and compare this model to the more traditional Cox proportional hazards model. Compared to previously studied models, we expect the ANN trained using Bayesian inference to perform better following its incorporation of Bayesian inference and neural networks.</p>
<p>First, we introduce <em>neural networks in a more general context</em>, then discuss <em>the neural networks approach to modeling survival data</em> and <em>how Bayesian inference has been introduced into these models to enhance their predictive capacity</em>. Next, we introduce an application of the Bayesian learning artificial neural network (BLNN) using an R package of a similar name applied in modeling the effect of identified differentially expressed genes on the survival of patients with primary bladder cancer. Lastly, we compare this model to the more traditional neural network for illustrative purposes and provide an extension code in Python.</p>
</div>
<div id="but-what-are-neural-networks" class="section level1" number="2">
<h1><span class="header-section-number" style="color:#4582ec">2</span> So, what are neural networks?</h1>
<p>With all the hype linked to this deep learning method in the recent past <span class="citation">(<a href="#ref-hastie2009elements" role="doc-biblioref">Hastie et al. 2009</a>)</span>, we provide a simplistic idea of what this method is. Defined: Neural networks are:</p>
<blockquote>
<p>“Computer systems with interconnected nodes designed like neurons to mimic the human brain in terms of intelligence. These networks use algorithms to discover hidden data structures and patterns, correlations, clusters, and classify them, learn and improve over time.”</p>
</blockquote>
<p>The idea is to take in simple functions as inputs and then allow these functions to build upon each other. The models are flexible enough to learn non-linear relationships rather than prescribing them as is in kernels or transformations. A neural network takes in an input vector of p features <span class="math inline">\(X=(X_1, X_2, \cdots , X_p)\)</span> and then creates a non-linear function to forecast an outcome variable, <span class="math inline">\(Y\)</span>. While varied statistical models such as Bayesian additive regression trees (BART) and random forests exist, neural networks have a structure that contrasts them from these other methods. Figure 1 shows a feed-forward neural network with an input layer consisting of 4 input features, <span class="math inline">\(X=(X_1, \cdots, X_4)\)</span>, a single hidden layer with 5 nodes <span class="math inline">\(A_1, \cdots, A_5\)</span>, a non-linear activation function, <span class="math inline">\(f(X)\)</span> (output layer), and the desired outcome, <span class="math inline">\(Y.\)</span></p>
<figure>
<img src="basic_neural_1.jpg" style="width:50%; display: block; margin-left: auto; margin-right: auto;">
<figcaption style="font-style: italic; text-align: center;" >
Image Source: Hastie et al. 2009
</figcaption>
</figure>
<p>The arrows show that the input layer is feeding into each of the nodes in the hidden layer which in turn feed into our activation function all the way to the outcome in a forward manner hence the name—“feed forward.” A general neural network model has the form:</p>
<p><span class="math display">\[\begin{align}
 f(X) &amp; = \beta_0 + \sum_{k=1}^{K} \beta_k h_k (X)\\
&amp; = \beta_0 + \sum_{k=1}^{K} \beta_k g(w_{k0} + \sum_{j=1}^{p}w_{kj}X_j)
\end{align}\]</span></p>
<p>In the first modeling step, the <span class="math inline">\(K\)</span> activations in the hidden layer are computed as functions of the features in the input layer, that is:</p>
<p><span class="math display">\[\begin{align}
A_k = h_k(X) &amp;= g(w_{k0} + \sum_{j=1}^{p}w_{kj}X_j)\\
\end{align}\]</span>
where <span class="math inline">\(g(z)\)</span> is the activation function specified. The <span class="math inline">\(K\)</span> activation functions from the hidden layer then feed their outputs into the output layer so that we have:</p>
<p><span class="math display">\[\begin{align}
 f(X) &amp; = \beta_0 + \sum_{k=1}^{K} \beta_k A_k
\end{align}\]</span></p>
<p>where <span class="math inline">\(K\)</span> in Figure 1 is 5. Parameters <span class="math inline">\(\beta_0, \cdots, \beta_K\)</span>, as well as, <span class="math inline">\(w_{10}, \cdots, w_{Kp}\)</span> are estimated from the data. Quite a number of options exist for the activation function, <span class="math inline">\(g(z)\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> The non-linearity of the activation function <span class="math inline">\(g(z)\)</span> allows the model to capture complex non-linear structures as well as interaction effects.</p>
</div>
<div id="the-artificial-neural-network-ann-approach-to-modeling-survival-data" class="section level1" number="3">
<h1><span class="header-section-number" style="color:#4582ec">3</span> The Artificial Neural Network (ANN) Approach to modeling survival data</h1>
<p>The BLNN implementation of Bayesian inference in artificial neural networks is based on the Cox PH-based neural model described by <span class="citation"><a href="#ref-sharaf2015two" role="doc-biblioref">Sharaf and Tsokos</a> (<a href="#ref-sharaf2015two" role="doc-biblioref">2015</a>)</span>. In particular, the idea is to build a predictive model for survival using a neural network with <span class="math inline">\(K\)</span> outputs. <span class="math inline">\(K\)</span> here defines the number of periods. Using this neural network architecture, Mani et al. estimated a hazard function where for each individual, we have a training vector a $1 K $ of hazard probabilities <span class="math inline">\((h_{ik})\)</span> defined as:
<span class="math display">\[ 
h_{ik}=
\begin{cases}
0 &amp; ~\textrm{if} ~ 1\leq k \leq K \\
1 &amp;~\textrm{if} ~ t \leq k \leq K ~ \textrm{and event = 1} \\
\frac{r_k}{n_k}~ \textrm{if}~ t \leq k \leq K ~ \textrm{and event = 0}
\end{cases}
\]</span>
where <span class="math inline">\(h_{ik}=0\)</span> if the event of interest did not occur (patient survived), <span class="math inline">\(h_{ik} =1\)</span> if event occurred at some time, <span class="math inline">\(t\)</span> and <span class="math inline">\(h_{ik}=\frac{r_k}{n_k}\)</span> if the subject is censored/ lost to follow -up during the course of the study, <span class="math inline">\(t&lt;K\)</span>. <span class="math inline">\(h_{ik}=\frac{r_k}{n_k}\)</span> is the Kaplan-Meier (KM) hazard estimate for time interval <span class="math inline">\(k\)</span> and <span class="math inline">\(r_k\)</span> and <span class="math inline">\(n_k\)</span> denote the number of events due to the risk factor of interest in time period <span class="math inline">\(k\)</span> and the number at risk in time interval <span class="math inline">\(k.\)</span> The neural network uses the logistic sigmoid activation function defined as:
<span class="math display">\[
\Phi (x) = \frac{1}{1+e^{-x}}
\]</span></p>
<p>The weights for this network are obtained through a minimization of the cross-entropy loss function<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>. Figure 2 shows the architecture of a feed-forward neural network based on the Cox PH model with an input layer consisting of <span class="math inline">\(p\)</span> covariates and a bias term, a single hidden layer with <span class="math inline">\(H\)</span> nodes, and a single bias term. Lastly, we have an output layer with <span class="math inline">\(K\)</span> units, which learn to estimate the hazard probabilities associated with each individual at each time interval. The network’s input layer feeds the hidden layer, which in turn feeds the output layer. The “feed-forward” naming convention is derived from this aspect of the architecture. The hazard estimates based on this neural network model are then converted to estimates of survival based on the survival function:</p>
<p><span class="math display">\[
S(t_k)=\prod_{l=1}^k (1-h(t_l))
\]</span></p>
<p>where <span class="math inline">\(k\)</span> denotes the disjoint intervals and <span class="math inline">\(l\)</span> the number of time periods in which the event occurred.</p>
<figure>
<center>
<img src="cox_net.png" style="width:100%">
<figcaption style="font-style: italic; text-align: center;" >
Image Source: Sharaf et al. 2020
</figcaption>
</center>
</figure>
</div>
<div id="bayesian-approach-to-inference-using-ann" class="section level1" number="4">
<h1><span class="header-section-number" style="color:#4582ec">4</span> Bayesian approach to inference using ANN</h1>
<p>This post focuses on inference using a two-layer feed-forward artificial neural network. Specifically, we describe the Bayesian learning neural networks implemented by <span class="citation"><a href="#ref-sharaf2020blnn" role="doc-biblioref">Sharaf et al.</a> (<a href="#ref-sharaf2020blnn" role="doc-biblioref">2020</a>)</span> on a neural network-based implementation of the Cox proportional hazard model described above. In training neural networks using conventional methodologies, the aim is to find a local minimum of the error function, an ideology that makes model selection rather difficult. Additionally, as described elsewhere by <span class="citation">(<a href="#ref-hastie2009elements" role="doc-biblioref">Hastie et al. 2009</a>)</span>, the training of neural networks presents such an issue as overfitting, a situation where, even though the model performs extremely well on the training data, it fails to generalize well on resampling or when applied on unobserved data. Overfitting has been linked to these models having too many weights such that they overfit at the global minimum of <span class="math inline">\(R\)</span> <span class="citation">(<a href="#ref-lawrence1997lessons" role="doc-biblioref">Lawrence, Giles, and Tsoi 1997</a>; <a href="#ref-hastie2009elements" role="doc-biblioref">Hastie et al. 2009</a>)</span>. According to <span class="citation"><a href="#ref-burden2008bayesian" role="doc-biblioref">Burden and Winkler</a> (<a href="#ref-burden2008bayesian" role="doc-biblioref">2008</a>)</span> :</p>
<blockquote>
<p>“Bayesian regularized artificial neural networks (BRANNs) are more robust than standard backpropagation nets and can reduce or eliminate the need for lengthy cross-validation.”</p>
</blockquote>
<p>In the Bayesian context, the idea is to use prior information about the distribution of the parameter of interest, update this information using the sample data and obtain a posterior distribution for the parameter, <span class="math inline">\(\theta\)</span>. BLNN tries to present Hamiltonian energy, <span class="math inline">\(H(w, p)= U(w)+K(p)\)</span> as a joint probability distribution of the neural network’s weights, <span class="math inline">\(w\)</span> and momentum, <span class="math inline">\(\textbf{p}\)</span>. Given independence between <span class="math inline">\(w\)</span> and <span class="math inline">\(\textbf{p}\)</span>, this joint probability is defined as:</p>
<p><span class="math display">\[
P(w, p) = (\frac{1}{z} exp^{-U(w)/z})(\frac{1}{T}exp^{-K(p)/T})
\]</span>
where:
<span class="math inline">\(U(w) =\)</span> the negative log-likelihood of the posterior distribution defined as <span class="math inline">\(U(w)=-log[p(w)L(w|D)]\)</span>
<span class="math inline">\(L(w|D) =\)</span> the likelihood function given the data <br>
<span class="math inline">\(K(p)=\sum_{i=1}^{d}(P_i^2)/(2m_i)\)</span> is the kinetic energy corresponding to the negative log-likelihood of the normal distribution with mean, <span class="math inline">\(\mu\)</span> and variance-covariance matrix with diagonal elements, <span class="math inline">\(M=(m_1, \cdots, m_d)\)</span> <br>
<span class="math inline">\(Z\)</span> and <span class="math inline">\(T\)</span> are the normalizing constants.</p>
<p>The algorithm is summarized as below:</p>
<div class="figure">
<img src="blnn_algorithm.jpg" alt="" />
<p class="caption" style="font-style: italic; text-align: center;" >Image Source: Sharaf et al. 2020</p>
</div>
<p>Details about the implementation of this method can be found <a href="https://rdrr.io/github/BLNNdevs/BLNN/#vignettes">here</a>.</p>
<p><span class="citation"><a href="#ref-sharaf2020blnn" role="doc-biblioref">Sharaf et al.</a> (<a href="#ref-sharaf2020blnn" role="doc-biblioref">2020</a>)</span> utilize a no-U-turn sampler (NUTS), an extension of Hamiltonian Monte-Carlo (HMC) that seeks to reduce the dependence on the number of step parameters used in HMC while retaining the efficiency in generating independent samples. The ANN is trained using both HMC and NUTS with dual averaging. The negative log-likelihood is replaced by network errors, and backpropagation is used to compute the gradients. Network errors and weights are assumed to be normally distributed with mean, <span class="math inline">\(\mu\)</span> but with a non-constant variance, <span class="math inline">\(\sigma^2\)</span>. The variance of the prior is known by the precision parameter, <span class="math inline">\(\tau = \frac{1}{\sigma^2}\)</span> aka the hyperparameters which are either assigned to fixed, fine-tuned values or re-estimated based on historical data. The list of hyperparameters allowed in the BLNN implementation is discussed elsewhere <span class="citation">(<a href="#ref-sharaf2020blnn" role="doc-biblioref">Sharaf et al. 2020</a>)</span>. The following section provides a sample application of BLNNs applied to real-world data.</p>
</div>
<div id="bayesian-based-neural-networks-for-modeling-survival-using-micro-array-data" class="section level1" number="5">
<h1><span class="header-section-number" style="color:#4582ec">5</span> Bayesian-based neural networks for modeling survival using micro-array data</h1>
<div id="introduction-and-data-description" class="section level2" number="5.1">
<h2><span class="header-section-number" style="color:#4582ec">5.1</span> Introduction and data description</h2>
<p>In our analyses in this section, we employ data consisting of 256 samples prepared using the Illumina Human-6 expression BeadChip Version 2.0 to identify DEGs and use bayesian neural networks to identify how these genes impact survival in patients with primary bladder cancer. The data relates to 165 primary bladder cancer samples and nine normal cells downloaded from the Gene Expression Omnibus (GEO) <span class="citation">(<a href="#ref-kim2010predictive" role="doc-biblioref">Kim et al. 2010</a>; <a href="#ref-okutse2021differential" role="doc-biblioref">Okutse and Nyongesa 2021</a>)</span>. Before we begin this demonstration, we first load the Bioconductor packages to be used in the anallysis. Details on the installation of this packages can be found elsewhere.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></p>
<p>After loading the required packages, we then download the data directly into our working environment and prepare it for analysis by performing a log transformation, creating a design matrix, and then fitting linear models for the identification of differentially expressed genes using empirical Bayes statistics for differential expression (eBayes) using the <code>limma</code> package <span class="citation">(<a href="#ref-ritchie2015limma" role="doc-biblioref">Ritchie et al. 2015</a>)</span>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#load the data from the GEO</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>gset <span class="ot">&lt;-</span> <span class="fu">getGEO</span>(<span class="st">&quot;GSE13507&quot;</span>, <span class="at">GSEMatrix =</span><span class="cn">TRUE</span>, <span class="at">AnnotGPL=</span><span class="cn">TRUE</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> (<span class="fu">length</span>(gset) <span class="sc">&gt;</span> <span class="dv">1</span>) idx <span class="ot">&lt;-</span> <span class="fu">grep</span>(<span class="st">&quot;GPL6102&quot;</span>, <span class="fu">attr</span>(gset, <span class="st">&quot;names&quot;</span>)) <span class="cf">else</span> idx <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>gset <span class="ot">&lt;-</span> gset[[idx]]</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># make proper column names to match toptable </span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">fvarLabels</span>(gset) <span class="ot">&lt;-</span> <span class="fu">make.names</span>(<span class="fu">fvarLabels</span>(gset))</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># group names for all samples</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>gsms <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;0000000000XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;</span>,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>               <span class="st">&quot;XXXXXXXXXXXXXXXXXX22222222222222222222222222222222&quot;</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>               <span class="st">&quot;22222222222222222222222222222222222222222222222222&quot;</span>,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>               <span class="st">&quot;22222222222222222222222222222222222222222222222222&quot;</span>,</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>               <span class="st">&quot;222222222222222222222222222222222XXXXXXXXXXXXXXXXX&quot;</span>,</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>               <span class="st">&quot;XXXXXX&quot;</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>sml <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nchar</span>(gsms)) { sml[i] <span class="ot">&lt;-</span> <span class="fu">substr</span>(gsms,i,i) }</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># eliminate samples marked as &quot;X&quot;</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>sel <span class="ot">&lt;-</span> <span class="fu">which</span>(sml <span class="sc">!=</span> <span class="st">&quot;X&quot;</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>sml <span class="ot">&lt;-</span> sml[sel]</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>gset <span class="ot">&lt;-</span> gset[ ,sel]</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># log2 transform</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="fu">exprs</span>(gset) <span class="ot">&lt;-</span> <span class="fu">log2</span>(<span class="fu">exprs</span>(gset))</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co"># set up the data and proceed with analysis</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>sml <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">&quot;G&quot;</span>, sml, <span class="at">sep=</span><span class="st">&quot;&quot;</span>)    <span class="co"># set group names</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>fl <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(sml)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>gset<span class="sc">$</span>description <span class="ot">&lt;-</span> fl</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>design <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(<span class="sc">~</span> description <span class="sc">+</span> <span class="dv">0</span>, gset)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(design) <span class="ot">&lt;-</span> <span class="fu">levels</span>(fl)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lmFit</span>(gset, design)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>cont.matrix <span class="ot">&lt;-</span> <span class="fu">makeContrasts</span>(G2<span class="sc">-</span>G0, <span class="at">levels=</span>design)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">contrasts.fit</span>(fit, cont.matrix)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">eBayes</span>(fit2, <span class="fl">0.01</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>tT <span class="ot">&lt;-</span> <span class="fu">topTable</span>(fit2, <span class="at">adjust=</span><span class="st">&quot;fdr&quot;</span>, <span class="at">sort.by=</span><span class="st">&quot;B&quot;</span>, <span class="at">number=</span><span class="cn">Inf</span>) </span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="co">#we can then save the list of 1000 DEGs</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="fu">saveRDS</span>(tT[<span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>,], <span class="st">&quot;deg.RDS&quot;</span>)</span></code></pre></div>
</div>
<div id="exploratory-data-analysis-and-deg-identification" class="section level2" number="5.2">
<h2><span class="header-section-number" style="color:#4582ec">5.2</span> Exploratory data analysis and DEG identification</h2>
<p>For our exploratory data analysis, we start by presenting a small sample of 10 up-and-down regulated genes in Table <a href="#tab:table1">5.1</a>. Here we notice that <em>CDC20</em> is one of the most significantly up-regulated genes between normal and primary bladder cancer samples (log-Fold Change = 0.472, Average Expression = 3.44, p&lt;0.001).</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">#top regulated genes</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>upregulated<span class="ot">&lt;-</span>tT[<span class="fu">which</span>(tT<span class="sc">$</span>logFC<span class="sc">&gt;</span><span class="dv">0</span>),][<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,]</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">#upregulated</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>upregulated<span class="ot">&lt;-</span><span class="fu">subset</span>(upregulated, <span class="at">select=</span><span class="fu">c</span>(<span class="st">&quot;Gene.ID&quot;</span>, <span class="st">&quot;Gene.symbol&quot;</span>,<span class="st">&quot;logFC&quot;</span>,<span class="st">&quot;AveExpr&quot;</span>,<span class="st">&quot;adj.P.Val&quot;</span>,<span class="st">&quot;B&quot;</span>))</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="do">##getting top 10 downregulated genes</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>downreg<span class="ot">&lt;-</span>tT[<span class="fu">which</span>(tT<span class="sc">$</span>logFC<span class="sc">&lt;</span><span class="dv">0</span>),][<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,]</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>downreg<span class="ot">&lt;-</span><span class="fu">subset</span>(downreg, <span class="at">select=</span><span class="fu">c</span>(<span class="st">&quot;Gene.ID&quot;</span>,<span class="st">&quot;Gene.symbol&quot;</span>,<span class="st">&quot;logFC&quot;</span>,<span class="st">&quot;AveExpr&quot;</span>,<span class="st">&quot;adj.P.Val&quot;</span>,<span class="st">&quot;B&quot;</span>))</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>deg<span class="ot">&lt;-</span><span class="fu">rbind</span>(upregulated, downreg) </span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(deg)<span class="ot">&lt;-</span><span class="cn">NULL</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>deg<span class="sc">%&gt;%</span> <span class="fu">kable</span>(<span class="at">format =</span> <span class="st">&quot;html&quot;</span>, </span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>             <span class="at">caption =</span> <span class="st">&quot;Top 10 up and down regulated genes in primary bladder cancer. The first 10 rows represent upregulated genes&quot;</span>,</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>             <span class="at">col.names =</span> <span class="fu">c</span>(<span class="st">&quot;Gene ID&quot;</span>, <span class="st">&quot;Gene Symbol&quot;</span>, <span class="st">&quot;logFC&quot;</span>, <span class="st">&quot;Average Expression&quot;</span>, <span class="st">&quot;Adjusted P-value&quot;</span>, <span class="st">&quot;B&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kable_styling</span>(<span class="at">full_width =</span> <span class="cn">FALSE</span>, <span class="at">latex_options =</span> <span class="fu">c</span>(<span class="st">&quot;HOLD_position&quot;</span>, <span class="st">&quot;stripped&quot;</span>, <span class="st">&quot;scale_down&quot;</span>), <span class="at">position =</span> <span class="st">&quot;left&quot;</span>)</span></code></pre></div>
<table class="table" style="width: auto !important; ">
<caption>
<span id="tab:table1">Table 5.1: </span>Top 10 up and down regulated genes in primary bladder cancer. The first 10 rows represent upregulated genes
</caption>
<thead>
<tr>
<th style="text-align:left;">
Gene ID
</th>
<th style="text-align:left;">
Gene Symbol
</th>
<th style="text-align:right;">
logFC
</th>
<th style="text-align:right;">
Average Expression
</th>
<th style="text-align:right;">
Adjusted P-value
</th>
<th style="text-align:right;">
B
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
991
</td>
<td style="text-align:left;">
CDC20
</td>
<td style="text-align:right;">
0.4715744
</td>
<td style="text-align:right;">
3.448492
</td>
<td style="text-align:right;">
0e+00
</td>
<td style="text-align:right;">
15.80790
</td>
</tr>
<tr>
<td style="text-align:left;">
5373
</td>
<td style="text-align:left;">
PMM2
</td>
<td style="text-align:right;">
0.1710619
</td>
<td style="text-align:right;">
3.338349
</td>
<td style="text-align:right;">
0e+00
</td>
<td style="text-align:right;">
15.27138
</td>
</tr>
<tr>
<td style="text-align:left;">
128239
</td>
<td style="text-align:left;">
IQGAP3
</td>
<td style="text-align:right;">
0.4125383
</td>
<td style="text-align:right;">
3.340340
</td>
<td style="text-align:right;">
0e+00
</td>
<td style="text-align:right;">
14.95455
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:right;">
0.2268436
</td>
<td style="text-align:right;">
3.233782
</td>
<td style="text-align:right;">
0e+00
</td>
<td style="text-align:right;">
13.79589
</td>
</tr>
<tr>
<td style="text-align:left;">
2932
</td>
<td style="text-align:left;">
GSK3B
</td>
<td style="text-align:right;">
0.1896154
</td>
<td style="text-align:right;">
3.377058
</td>
<td style="text-align:right;">
1e-07
</td>
<td style="text-align:right;">
12.82440
</td>
</tr>
<tr>
<td style="text-align:left;">
51537
</td>
<td style="text-align:left;">
MTFP1
</td>
<td style="text-align:right;">
0.2168117
</td>
<td style="text-align:right;">
3.393753
</td>
<td style="text-align:right;">
2e-07
</td>
<td style="text-align:right;">
12.03233
</td>
</tr>
<tr>
<td style="text-align:left;">
51203
</td>
<td style="text-align:left;">
NUSAP1
</td>
<td style="text-align:right;">
0.3719761
</td>
<td style="text-align:right;">
3.344562
</td>
<td style="text-align:right;">
3e-07
</td>
<td style="text-align:right;">
11.28112
</td>
</tr>
<tr>
<td style="text-align:left;">
7153
</td>
<td style="text-align:left;">
TOP2A
</td>
<td style="text-align:right;">
0.4546483
</td>
<td style="text-align:right;">
3.341922
</td>
<td style="text-align:right;">
4e-07
</td>
<td style="text-align:right;">
11.09297
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:right;">
0.0471829
</td>
<td style="text-align:right;">
2.866252
</td>
<td style="text-align:right;">
5e-07
</td>
<td style="text-align:right;">
10.82223
</td>
</tr>
<tr>
<td style="text-align:left;">
2810
</td>
<td style="text-align:left;">
SFN
</td>
<td style="text-align:right;">
0.3100817
</td>
<td style="text-align:right;">
3.493720
</td>
<td style="text-align:right;">
8e-07
</td>
<td style="text-align:right;">
10.18284
</td>
</tr>
<tr>
<td style="text-align:left;">
9890
</td>
<td style="text-align:left;">
PLPPR4
</td>
<td style="text-align:right;">
-0.3804426
</td>
<td style="text-align:right;">
2.891050
</td>
<td style="text-align:right;">
0e+00
</td>
<td style="text-align:right;">
50.66289
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:right;">
-0.0622618
</td>
<td style="text-align:right;">
2.808016
</td>
<td style="text-align:right;">
0e+00
</td>
<td style="text-align:right;">
42.95524
</td>
</tr>
<tr>
<td style="text-align:left;">
54360
</td>
<td style="text-align:left;">
CYTL1
</td>
<td style="text-align:right;">
-0.2690733
</td>
<td style="text-align:right;">
2.886378
</td>
<td style="text-align:right;">
0e+00
</td>
<td style="text-align:right;">
41.75689
</td>
</tr>
<tr>
<td style="text-align:left;">
11126
</td>
<td style="text-align:left;">
CD160
</td>
<td style="text-align:right;">
-0.1475023
</td>
<td style="text-align:right;">
2.853721
</td>
<td style="text-align:right;">
0e+00
</td>
<td style="text-align:right;">
38.93153
</td>
</tr>
<tr>
<td style="text-align:left;">
121601
</td>
<td style="text-align:left;">
ANO4
</td>
<td style="text-align:right;">
-0.1705039
</td>
<td style="text-align:right;">
2.827650
</td>
<td style="text-align:right;">
0e+00
</td>
<td style="text-align:right;">
36.38865
</td>
</tr>
<tr>
<td style="text-align:left;">
5126
</td>
<td style="text-align:left;">
PCSK2
</td>
<td style="text-align:right;">
-0.1268787
</td>
<td style="text-align:right;">
2.823028
</td>
<td style="text-align:right;">
0e+00
</td>
<td style="text-align:right;">
34.23361
</td>
</tr>
<tr>
<td style="text-align:left;">
9745
</td>
<td style="text-align:left;">
ZNF536
</td>
<td style="text-align:right;">
-0.2280635
</td>
<td style="text-align:right;">
2.898368
</td>
<td style="text-align:right;">
0e+00
</td>
<td style="text-align:right;">
34.04707
</td>
</tr>
<tr>
<td style="text-align:left;">
55022
</td>
<td style="text-align:left;">
PID1
</td>
<td style="text-align:right;">
-0.3029122
</td>
<td style="text-align:right;">
2.899585
</td>
<td style="text-align:right;">
0e+00
</td>
<td style="text-align:right;">
33.04130
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:right;">
-0.2550758
</td>
<td style="text-align:right;">
2.837756
</td>
<td style="text-align:right;">
0e+00
</td>
<td style="text-align:right;">
32.74756
</td>
</tr>
<tr>
<td style="text-align:left;">
10699
</td>
<td style="text-align:left;">
CORIN
</td>
<td style="text-align:right;">
-0.1867473
</td>
<td style="text-align:right;">
2.868771
</td>
<td style="text-align:right;">
0e+00
</td>
<td style="text-align:right;">
32.25715
</td>
</tr>
</tbody>
</table>
<p>In Figure <a href="#fig:mean-difference">5.1</a> we visualize the differentially expressed genes in this sample using a mean-difference plot highlighting the genes depending on whether they were significantly up or down regulated.</p>
<div class="figure"><span style="display:block;" id="fig:mean-difference"></span>
<img src="index_files/figure-html/mean-difference-1.png" alt="A mean-difference plot showing the statistically significantly up and down regulated genes in primary bladder cancer relative to normal bladder cells."  />
<p class="caption">
Figure 5.1: A mean-difference plot showing the statistically significantly up and down regulated genes in primary bladder cancer relative to normal bladder cells.
</p>
</div>
<p>Additionally, we perform a basic functional enrichment analysis using Gene Ontology (GO) enrichment analysis and a small fraction of the DEGs (n = 300 genes) to identify the pathways where this statistically significantly differentially expressed genes are enriched. Here, we use a basic barplot to visualize the most common GO terms. We note in Figure <a href="#fig:enrich">5.2</a> that these genes are involved in a number of biological functions including synapse organization as well as junction assembly and neural migration.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">#barplots of the similar results showing only 20 enrichment categories</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(edo, <span class="at">showCategory=</span><span class="dv">10</span>, <span class="at">cex.names=</span><span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:enrich"></span>
<img src="index_files/figure-html/enrich-1.png" alt="Sample Enrichment analysis results using the Gene Ontology (GO) enrichment analysis."  />
<p class="caption">
Figure 5.2: Sample Enrichment analysis results using the Gene Ontology (GO) enrichment analysis.
</p>
</div>
</div>
</div>
<div id="artificial-neural-networks-ann-with-bayesian-learning" class="section level1" number="6">
<h1><span class="header-section-number" style="color:#4582ec">6</span> Artificial Neural Networks (ANN) with Bayesian Learning</h1>
<p>In this analysis, because of the computationally intensive nature of training neural network models using Bayesian learning and simulation methodology such as the NUTS algorithm, we restrict analyses to only a small sample of the genetic information available in the dataset. We load a sample of n= 150 genetic information for analysis using BLNN.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">#load the bayesian learning package</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(BLNN)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">#load the saved train and test data files</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>trainy<span class="ot">&lt;-</span><span class="fu">readRDS</span>(<span class="st">&quot;trainy.RDS&quot;</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>trainx<span class="ot">&lt;-</span><span class="fu">readRDS</span>(<span class="st">&quot;trainxx.RDS&quot;</span>)</span></code></pre></div>
<p>First, we build up an object of class <code>BLNN</code> using the <code>BLNN_Build()</code> declaring the number of input features in the input layer (<code>ncol(trainx)</code>), the number of nodes in the output layer <code>nout</code>, the number of nodes in the hidden layer, the activation function, the cost function, the output function, and the weights for the input features, bias terms, as well as hidden nodes. Details on all parameters accepted as inputs by the function can be found <a href="https://rdrr.io/github/BLNNdevs/BLNN/man/BLNN_Build.html">here</a>. The hyper-parameters used here are arbitrarily selected and are re-estimated through an evidence procedure during the model training process.</p>
<p>The created <code>BLNN</code> object is then trained either using evidence or not. Here, we first use the <code>nnet</code> package as a baseline and compare this neural network model performance to the BLNN model with evidence and another neural network fitted to the data using the same architecture described, but without evidence. We use two chains in our learning process in order to develop an understanding of the parameter space while varying the starting values. The training function <code>BLNN_Train()</code> function is supplied with random normal weights from the standard normal distribution. For each chain, however, we use a predefined method of generating weights.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">#set the hyperparameters;  change this to evaluate</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co">#network weights</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>n.par<span class="ot">&lt;-</span><span class="fu">length</span>(<span class="fu">BLNN_GetWts</span>(survObj))</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">#number of desired chains</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>chains<span class="ot">&lt;-</span><span class="dv">2</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co">#initials weight values</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>initials<span class="ot">&lt;-</span><span class="fu">lapply</span>(<span class="dv">1</span><span class="sc">:</span>chains, <span class="cf">function</span>(i) <span class="fu">rnorm</span>(n.par, <span class="dv">0</span>, <span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(n.par)))</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">#using nnet as baseline</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(nnet)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>nnetBasesline<span class="ot">&lt;-</span><span class="fu">nnet</span>(trainx, trainy, <span class="at">size=</span> <span class="dv">75</span>, <span class="at">maxit =</span> <span class="dv">1000</span>, <span class="at">MaxNWts =</span> <span class="dv">15000</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>nnetPredictions<span class="ot">&lt;-</span><span class="fu">predict</span>(nnetBasesline)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>NET.abserror<span class="ot">&lt;-</span><span class="fu">sum</span>(<span class="fu">abs</span>(trainy<span class="sc">-</span>nnetPredictions)) <span class="co">#0.0736</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>NET.error<span class="ot">&lt;-</span>nnetBasesline<span class="sc">$</span>value <span class="co">#7.97425e-05</span></span></code></pre></div>
<p>First, we train our feed-forward neural network model without using evidence by passing the function the <code>BLNN</code> object, the dataframe with covariates, the response vector, <span class="math inline">\(y\)</span>, the number of iterations and use parallelization to leverage the power of multi-core computing. Additional arguments to this function can be found elsewhere.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">#variance for the moments</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>m1<span class="ot">&lt;-</span><span class="fu">rep</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>, n.par)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co">#training the model</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>trainx<span class="ot">&lt;-</span><span class="fu">scale</span>(trainx)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>survNUTS<span class="ot">&lt;-</span><span class="fu">BLNN_Train</span>(survObj, <span class="at">x=</span>trainx, <span class="at">y=</span>trainy,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>                     <span class="at">iter =</span> <span class="dv">2000</span>, <span class="at">thin =</span> <span class="dv">10</span>, <span class="at">warmup =</span> <span class="dv">400</span>,</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>                     <span class="at">init =</span> initials, <span class="at">chains =</span> chains,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>                     <span class="at">parallel =</span> <span class="cn">TRUE</span>, <span class="at">cores =</span> <span class="dv">2</span>,</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>                     <span class="at">algorithm =</span> <span class="st">&quot;NUTS&quot;</span>, <span class="at">evidence =</span> <span class="cn">FALSE</span>,</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>                     <span class="at">control =</span> <span class="fu">list</span>(<span class="at">adapt_delta=</span><span class="fl">0.99</span>, <span class="at">momentum_mass=</span>m1, </span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">stepsize=</span> <span class="dv">1</span>, <span class="at">gamma=</span><span class="fl">0.05</span>, <span class="at">to=</span><span class="dv">100</span>, <span class="at">useDA=</span><span class="cn">TRUE</span>,</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">max_treedepth=</span><span class="dv">20</span>))</span></code></pre></div>
<p>We check our values of Rhat to ensure that they are below one, and that we have larger values for effective sample size (minimum 50 each) before we can update each of our networks with the newly sampled parameters.</p>
<p>The next neural network model is built on this data but with the evidence option allowed to enable re-estimation of the model hyper-parameters using historical data or the matrix of covariates, <span class="math inline">\(x\)</span> if the historical data is not available.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">#bayesian learning with evidence used in re-estimating hyper-parameters</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>survNUTS.ev<span class="ot">&lt;-</span><span class="fu">BLNN_Train</span>(survObj, <span class="at">x=</span>trainx, <span class="at">y=</span>trainy,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>                     <span class="at">iter =</span> <span class="dv">5000</span>, <span class="at">thin =</span> <span class="dv">10</span>, <span class="at">warmup =</span> <span class="dv">400</span>,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>                     <span class="at">init =</span> initials, <span class="at">chains =</span> chains,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>                     <span class="at">parallel =</span> <span class="cn">TRUE</span>, <span class="at">cores =</span> <span class="dv">2</span>,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>                     <span class="at">algorithm =</span> <span class="st">&quot;NUTS&quot;</span>, <span class="at">evidence =</span> <span class="cn">TRUE</span>,</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>                     <span class="at">control =</span> <span class="fu">list</span>(<span class="at">adapt_delta=</span><span class="fl">0.99</span>, <span class="at">momentum_mass=</span>m1, </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">stepsize =</span> <span class="dv">5</span>, <span class="at">gamma =</span> <span class="fl">0.05</span>, <span class="at">to=</span><span class="dv">100</span>, <span class="at">useDA=</span><span class="cn">TRUE</span>,</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">max_treedepth=</span><span class="dv">20</span>))</span></code></pre></div>
<p>After training, we can then update the trained neural network models with the values sampled from the learning process using the <code>BLNN_Update()</code> function. If evidence was used in training the hyper-parameters will be updated as well.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co">#updating the parameters after the learning process</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="do">##update the no evidence neural network</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>survNUTS<span class="ot">&lt;-</span><span class="fu">BLNN_Update</span>(survObj, survNUTS)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="do">##update the evidence neural network</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>survNUTS.ev<span class="ot">&lt;-</span><span class="fu">BLNN_Update</span>(survObj, survNUTS.ev)</span></code></pre></div>
<p>After training the models, we can then update the trained neural network models with the values sampled from the learning process using the <code>BLNN_Update()</code> function. If evidence was used in training the hyper-parameters will be updated as well.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">#making predictions using these models</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="do">##predictions using no evidence</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>survpred<span class="ot">&lt;-</span><span class="fu">BLNN_Predict</span>(survNUTS, trainx, trainy)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="do">##predictions using bayesian learning</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>survpred.ev<span class="ot">&lt;-</span><span class="fu">BLNN_Predict</span>(survNUTS.ev, trainx, trainy)</span></code></pre></div>
<p>To evaluate our models, we create a tables of all evaluation metrics including all the models trained in this post, for illustrative purposes. Here we report the errors associated with the use of these models in survival modeling in Table <a href="#tab:bb">6.1</a>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Model evaluations</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="do">##extract the errors in the classification</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>errors<span class="ot">&lt;-</span><span class="fu">c</span>(survpred<span class="sc">$</span>Errors<span class="sc">$</span>Total, survpred.ev<span class="sc">$</span>Errors<span class="sc">$</span>Total, nnetBasesline<span class="sc">$</span>value)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">#print out the model evaluations</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>OutTab<span class="ot">&lt;-</span><span class="fu">data.frame</span>(errors)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(OutTab)<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="st">&quot;NUTS (no evidence)&quot;</span>, <span class="st">&quot;NUTS (with evidence)&quot;</span>, <span class="st">&quot;NNET&quot;</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="fu">saveRDS</span>(OutTab, <span class="st">&quot;outTab.RDS&quot;</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="fu">write.csv</span>(OutTab, <span class="st">&quot;OutTab.csv&quot;</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>OutTab <span class="sc">%&gt;%</span> <span class="fu">kable</span>(<span class="at">format =</span> <span class="st">&quot;html&quot;</span>, <span class="at">caption =</span> <span class="st">&quot;Artificial Neural Network Model Comparisons&quot;</span>) <span class="sc">%&gt;%</span> <span class="fu">kable_styling</span>(<span class="at">full_width =</span> <span class="cn">FALSE</span>, <span class="at">latex_options =</span> <span class="fu">c</span>(<span class="st">&quot;HOLD_position&quot;</span>, <span class="st">&quot;stripped&quot;</span>, <span class="st">&quot;scale_down&quot;</span>), <span class="at">position =</span> <span class="st">&quot;left&quot;</span>)</span></code></pre></div>
<table class="table" style="width: auto !important; ">
<caption>
<span id="tab:bb">Table 6.1: </span>Artificial Neural Network Model Comparisons
</caption>
<tbody>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Error
</td>
</tr>
<tr>
<td style="text-align:left;">
NUTS (no evidence)
</td>
<td style="text-align:left;">
1.00E-04
</td>
</tr>
<tr>
<td style="text-align:left;">
NUTS (with evidence)
</td>
<td style="text-align:left;">
7.97E-07
</td>
</tr>
<tr>
<td style="text-align:left;">
NNET
</td>
<td style="text-align:left;">
7.97E-05
</td>
</tr>
</tbody>
</table>
</div>
<div id="discussion-and-conclusion" class="section level1" number="7">
<h1><span class="header-section-number" style="color:#4582ec">7</span> Discussion and Conclusion</h1>
<p>In this extension post, we have explored Bayesian learning in the context of artificial neural networks for modeling survival data, using micro array gene expression data for our case. In particular, we have presented the main ideas behind why artificial neural networks using Bayesian learning present an alternative to modeling survival outcomes given the complexity presented by censored data in survival modeling. The post has presented a simplistic introduction to neural networks, focusing predominantly on the feed-forward neural network which is later implemented on the application problem. We later shift to the neural network approach to modeling survival data where we discuss the Cox PH-based ANNs and how this model has been improved to learn hyper-parameters from a posterior distribution derived from prior information (evidence) and the data.</p>
<p>For brevity, and purposes of demonstration, we present the errors associated with each models trained in this post and show that using evidence results in a reduction in the total error (see Table <a href="#tab:bb">6.1</a>). However, a major issue that we encountered during the course of training these models was the high computational time it took to run the models, even with parallel processing. While the NUTs algorithm and the HMC algorithm employed by the BLNN package are promising, the sampling methods take an exorbitant amount of time to run, especially when the idea is to sample the values of the hyperparameters using historical data (or evidence). <span class="citation"><a href="#ref-sharaf2020blnn" role="doc-biblioref">Sharaf et al.</a> (<a href="#ref-sharaf2020blnn" role="doc-biblioref">2020</a>)</span> note that “Bayesian learning for ANN requires longer learning time when compared with conventional algorithms such as BFGS or gradient descent” something that holds true based on our implementation in this post.</p>
</div>
<div id="python-extension" class="section level1" number="8">
<h1><span class="header-section-number" style="color:#4582ec">8</span> Python extension</h1>
<p>In an extension to this post, we show an implementation of Bayesian learning for neural networks using a simulated dataset in Python is available. The method includes the estimation of the uncertatinty associated with survival estimates which is of great value in the context of predictive modeling. The post can be found <a href="https://okutse.github.io">here</a>. After modeling and identifying the genes that impact survival in primary bladder cancer patients, the genes can then be annotated and encoded based on their enrichment level as either high, medium, or low and survival curves plotted to understand the effect these varied expression profiles have on survival.</p>
</div>
<div id="data-availability-and-analysis-scripts" class="section level1" number="9">
<h1><span class="header-section-number" style="color:#4582ec">9</span> Data availability and analysis scripts</h1>
<p>All data used in this extension post is available online. The cleaned data analysis files used in this post, including the <a href="https://github.com/okutse/bayesian-networks/blob/master/README.R">R Script</a> generated from the <code>.Rmd</code> file using <code>knitr::purl()</code> can be found at our Github repository.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6, </sup></a><a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
</div>
<div id="references" class="section level1" number="10">
<h1><span class="header-section-number" style="color:#4582ec">10</span> References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-burden2008bayesian" class="csl-entry">
Burden, Frank, and Dave Winkler. 2008. <span>“Bayesian Regularization of Neural Networks.”</span> <em>Artificial Neural Networks</em>, 23–42.
</div>
<div id="ref-feng2021bdnnsurv" class="csl-entry">
Feng, Dai, and Lili Zhao. 2021. <span>“BDNNSurv: Bayesian Deep Neural Networks for Survival Analysis Using Pseudo Values.”</span> <em>arXiv Preprint arXiv:2101.03170</em>.
</div>
<div id="ref-hastie2009elements" class="csl-entry">
Hastie, Trevor, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Vol. 2. Springer.
</div>
<div id="ref-kim2010predictive" class="csl-entry">
Kim, Wun-Jae, Eun-Jung Kim, Seon-Kyu Kim, Yong-June Kim, Yun-Sok Ha, Pildu Jeong, Min-Ju Kim, et al. 2010. <span>“Predictive Value of Progression-Related Gene Classifier in Primary Non-Muscle Invasive Bladder Cancer.”</span> <em>Molecular Cancer</em> 9 (1): 1–9.
</div>
<div id="ref-lawrence1997lessons" class="csl-entry">
Lawrence, Steve, C Lee Giles, and Ah Chung Tsoi. 1997. <span>“Lessons in Neural Network Training: Overfitting May Be Harder Than Expected.”</span> In <em>AAAI/IAAI</em>, 540–45. Citeseer.
</div>
<div id="ref-okutse2021differential" class="csl-entry">
Okutse, Amos, and Kelvin Nyongesa. 2021. <span>“Differential Expression Analysis for the Identification of Survival Associated Genes in Primary Bladder Cancer Using Microarray Data.”</span> <em>International Journal of Undergraduate Research and Creative Activities</em> 13 (1).
</div>
<div id="ref-ritchie2015limma" class="csl-entry">
Ritchie, Matthew E, Belinda Phipson, DI Wu, Yifang Hu, Charity W Law, Wei Shi, and Gordon K Smyth. 2015. <span>“Limma Powers Differential Expression Analyses for RNA-Sequencing and Microarray Studies.”</span> <em>Nucleic Acids Research</em> 43 (7): e47–47.
</div>
<div id="ref-sharaf2015two" class="csl-entry">
Sharaf, Taysseer, and Chris P Tsokos. 2015. <span>“Two Artificial Neural Networks for Modeling Discrete Survival Time of Censored Data.”</span> <em>Advances in Artificial Intelligence</em> 2015.
</div>
<div id="ref-sharaf2020blnn" class="csl-entry">
Sharaf, Taysseer, Theren Williams, Abdallah Chehade, and Keshav Pokhrel. 2020. <span>“BLNN: An r Package for Training Neural Networks Using Bayesian Inference.”</span> <em>SoftwareX</em> 11: 100432.
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Censoring refers to a concept in survival analysis where the actual time to event is unknown due to such reasons as the loss to follow up, withdrawals, or an exact unknown time of event. In right censoring, the event of interest occurs after the end of the experiment or study, whereas in left censoring, the event occurs before the onset of the study. Interval censoring is when the actual survival time is bounded between some interval<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Common activation functions include the sigmoid activation function favored in classification problems, the rectified linear unit (ReLU) favored in linear regression problems, tanh, and leaky ReLU.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p> formula defined here<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p><a href="https://www.bioconductor.org/install/" class="uri">https://www.bioconductor.org/install/</a><a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p><a href="https://rdrr.io/github/BLNNdevs/BLNN/man/BLNN_Train.html" class="uri">https://rdrr.io/github/BLNNdevs/BLNN/man/BLNN_Train.html</a><a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p><a href="https://github.com/okutse/bayesian-networks" class="uri">https://github.com/okutse/bayesian-networks</a><a href="#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p><a href="https://github.com/nlee100/bayesian-neuralnetworks" class="uri">https://github.com/nlee100/bayesian-neuralnetworks</a><a href="#fnref7" class="footnote-back">↩︎</a></p></li>
</ol>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "show");
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML.js";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
